{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem HANGMAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement :\n",
    "<hr>\n",
    " One of the interesting problems in NLP for healthcare is in handling medical coded term associations and spelling errors.So we are going to mimic the problem with hangman / dump charades like NLP word games. Your ML model should predict the correct word from an intentionally obscured word and its description (Hint). Our evaluation set will have incomplete words and their descriptions.\n",
    "<hr>\n",
    "\n",
    "## Example\n",
    "<hr>\n",
    "Input masked word = DEM_G_A_HY and Description = is the statistical study of populations, especially human beings.\n",
    "<br>\n",
    "Model prediction/output = DEMOGRAPHY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install numpy\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download wordnet and stopwords from nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Directories for Loading Data\n",
    "curr_dir = os.getcwd()\n",
    "target_words_dir = 'Targets.txt'\n",
    "target_txt = open(os.path.join(curr_dir,target_words_dir),\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_words(file_dir):\n",
    "    target_txt = open(file_dir,\"r\")\n",
    "    words = target_txt.readlines()\n",
    "    return([word[:-1].lower() for word in words])\n",
    "\n",
    "def get_dataframe(target_words):\n",
    "    data = {\"words\":[],\"description\":[]}\n",
    "    for word in target_words:\n",
    "        try:\n",
    "            synset = wordnet.synsets(word)         \n",
    "            data[\"description\"].append(synset[0].definition())\n",
    "            data[\"words\"].append(word)\n",
    "        except:pass\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_words --> List of target words\n",
    "target_words = np.array(get_target_words(target_words_dir))\n",
    "\n",
    "#Returns training dataset\n",
    "data = get_dataframe(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kitty</td>\n",
       "      <td>the combined stakes of the betters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lookup</td>\n",
       "      <td>an operation that determines whether one or mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>otiose</td>\n",
       "      <td>serving no useful purpose; having no excuse fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gallant</td>\n",
       "      <td>a man who is much concerned with his dress and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     words                                        description\n",
       "0    kitty                 the combined stakes of the betters\n",
       "1   lookup  an operation that determines whether one or mo...\n",
       "2   otiose  serving no useful purpose; having no excuse fo...\n",
       "3  gallant  a man who is much concerned with his dress and..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords  \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Class\n",
    "class PreProcess:\n",
    "    tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "    max_sequence_len = None\n",
    "\n",
    "    def __init__(self,corpus):\n",
    "        self.corpus = self.remove_stop_words(corpus)\n",
    "        self.tokenizer.fit_on_texts(corpus)\n",
    "            \n",
    "    def remove_stop_words(self,data):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        for i in range(len(data)):\n",
    "            sent_lst = list(data[i].split(\" \"))\n",
    "            sent_lst = filter(lambda x: x not in stop_words, sent_lst)\n",
    "            data[i] = ' '.join([str(wrd) for wrd in sent_lst])\n",
    "        return data\n",
    "            \n",
    "    def tokenize(self,data):\n",
    "        data = self.remove_stop_words(data)\n",
    "        seq = self.tokenizer.texts_to_sequences(data)\n",
    "        if(self.max_sequence_len == None):self.max_sequence_len = max([len(x) for x in seq])        \n",
    "        seq_padded = pad_sequences(seq,maxlen=self.max_sequence_len)\n",
    "        return(seq_padded)\n",
    "    \n",
    "    def get_word_index(self):\n",
    "        return(self.tokenizer.word_index)\n",
    "    \n",
    "    def get_seq_length(self):\n",
    "        return self.max_sequence_len\n",
    "    \n",
    "    def get_vocab_length(self):\n",
    "        return len(self.tokenizer.word_index) + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Meaning of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_pro_X  --> preprocessing object for Meanings/ Description\n",
    "\n",
    "pre_pro_X = PreProcess(data['description'])\n",
    "X = np.array(pre_pro_X.tokenize(data['description']))\n",
    "\n",
    "max_seq_length = pre_pro_X.get_seq_length()\n",
    "vocab_length = pre_pro_X.get_vocab_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_pro_y  --> preprocessing object for target words\n",
    "\n",
    "pre_pro_y = PreProcess(data['words'])\n",
    "y = np.array(pre_pro_y.tokenize(data['words']))\n",
    "\n",
    "total_target_words = pre_pro_y.get_vocab_length()\n",
    "ys = tf.keras.utils.to_categorical(y, num_classes=total_target_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  LSTM Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "## Embedding Layer\n",
    "model.add(Embedding(vocab_length, 100, input_length=36))\n",
    "\n",
    "## LSTM Layer\n",
    "model.add(Bidirectional(LSTM(256,activation='relu')))\n",
    "\n",
    "## Output Layer\n",
    "model.add(Dense(total_target_words,activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"ADAM\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "672/672 [==============================] - 139s 207ms/step - loss: 10.4673 - accuracy: 0.0036\n",
      "Epoch 2/20\n",
      "  7/672 [..............................] - ETA: 2:04 - loss: 10.3034 - accuracy: 0.0045  "
     ]
    }
   ],
   "source": [
    "earlystop = EarlyStopping(monitor='accuracy', min_delta=0, patience=1, verbose=0, mode='auto')\n",
    "history = model.fit(X, ys, epochs=20, verbose=1,callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_save(model_dir):\n",
    "    model.save(model_dir);\n",
    "    \n",
    "model_save('LSTM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_dir):\n",
    "    return(tf.keras.load_model(model_dir))\n",
    "\n",
    "#model = load_model(\"LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history,'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting for Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predict:\n",
    "    \n",
    "    def __init__(self,target_words,model,pre_pro_X,pre_pro_y):\n",
    "        self.target_words =  target_words\n",
    "        self.model        =  model\n",
    "        self.pre_pro_X    =  pre_pro_X\n",
    "        self.pre_pro_y    =  pre_pro_y\n",
    "        \n",
    "        \n",
    "    # get_words_set() -> returns the words which are similar to the masked word       \n",
    "    def get_words_set(self,pred_word,dashed,wrd_len):\n",
    "        words_set = filter(lambda word : nltk.edit_distance(word, pred_word) ==  dashed and wrd_len ==  len(word),\n",
    "                          self.target_words)\n",
    "        return words_set\n",
    "    \n",
    "        \n",
    "    # predict_word(self, wrd, pred) -> returns predicted word    \n",
    "    def predict_word(self,wrd,preds):\n",
    "        wrd_lst = wrd.split(\" \")\n",
    "        wrd_conv = ''.join(x for x in filter(lambda x: x!='_', wrd_lst))\n",
    "        \n",
    "        words_set = self.get_words_set(wrd_conv,wrd_lst.count('_'),len(wrd_lst))\n",
    "        \n",
    "        target_wrds_set = list(self.pre_pro_y.tokenizer.word_index.keys())\n",
    "        target_wrds_inxs, selective_preds = [],[]\n",
    "\n",
    "        for word in words_set:\n",
    "            try:target_wrds_inxs.append(word)\n",
    "            except:pass    \n",
    "\n",
    "        for inx in target_wrds_inxs:\n",
    "            try:selective_preds.append(preds[target_wrds_set.index(inx)])\n",
    "            except:pass\n",
    "\n",
    "        try:\n",
    "            max_prob = max(selective_preds)\n",
    "        except:\n",
    "            return target_wrds_set[preds.index(max(preds))];\n",
    "        \n",
    "        return(target_wrds_set[preds.index(max_prob)])\n",
    "                \n",
    "    \n",
    "    # predict_words(self, words_dataframe) -> returns predicted values in dict\n",
    "    def predict_words(self,words_dataframe):\n",
    "        final_preds = {\"Masked\":[],\"preds\":[],\"description\":[]}\n",
    "        \n",
    "        for index,row in words_dataframe.iterrows():\n",
    "                               \n",
    "            test_desc_encoded = self.pre_pro_X.tokenize([row['Meaning']])\n",
    "            \n",
    "            preds = list(self.model.predict(test_desc_encoded).reshape(-1))\n",
    "            \n",
    "            predicted_word = self.predict_word(row['Masked'],preds)\n",
    "            \n",
    "            if(preds!=False):\n",
    "                final_preds[\"Masked\"].append(row[\"Masked\"])\n",
    "                final_preds[\"description\"].append(row[\"Meaning\"])\n",
    "            \n",
    "                final_preds[\"preds\"].append(self.predict_word(row[\"Masked\"],preds))\n",
    "        \n",
    "        return final_preds\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = Predict(target_words, model, pre_pro_X, pre_pro_y)\n",
    "test_df = pd.read_csv(os.path.join(curr_dir,\"eval.csv\"))\n",
    "preds_test = pd.DataFrame(predict.predict_words(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(preds_test.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preds(preds,save_dir):\n",
    "    preds.to_csv(save_dir)\n",
    "\n",
    "save_preds(preds_test,\"predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

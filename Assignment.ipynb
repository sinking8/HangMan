{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ashwin\\anaconda3\\envs\\tf\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\ashwin\\anaconda3\\envs\\tf\\lib\\site-packages (from nltk) (1.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ashwin\\anaconda3\\envs\\tf\\lib\\site-packages (from nltk) (4.56.2)\n",
      "Requirement already satisfied: click in c:\\users\\ashwin\\anaconda3\\envs\\tf\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\ashwin\\anaconda3\\envs\\tf\\lib\\site-packages (from nltk) (2020.11.13)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directories\n",
    "curr_dir = os.getcwd()\n",
    "target_words_dir = 'Targets.txt'\n",
    "target_txt = open(os.path.join(curr_dir,target_words_dir),\"r\")\n",
    "\n",
    "def get_target_words(file_dir):\n",
    "    target_txt = open(file_dir,\"r\")\n",
    "    words = target_txt.readlines()\n",
    "    return([word[:-1].lower() for word in words])\n",
    "\n",
    "def get_dataframe(target_words):\n",
    "    data = {\"words\":[],\"description\":[]}\n",
    "    for word in target_words:\n",
    "        try:\n",
    "            synset = wordnet.synsets(word)         \n",
    "            data[\"description\"].append(synset[0].definition())\n",
    "            data[\"words\"].append(word)\n",
    "        except:pass\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_words = np.array(get_target_words(target_words_dir))\n",
    "data = get_dataframe(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kitty</td>\n",
       "      <td>the combined stakes of the betters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lookup</td>\n",
       "      <td>an operation that determines whether one or mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>otiose</td>\n",
       "      <td>serving no useful purpose; having no excuse fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gallant</td>\n",
       "      <td>a man who is much concerned with his dress and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     words                                        description\n",
       "0    kitty                 the combined stakes of the betters\n",
       "1   lookup  an operation that determines whether one or mo...\n",
       "2   otiose  serving no useful purpose; having no excuse fo...\n",
       "3  gallant  a man who is much concerned with his dress and..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from nltk.corpus import stopwords  \n",
    "\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# import tensorflow as tf\n",
    "\n",
    "# from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcess:\n",
    "    tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "    max_sequence_len = None\n",
    "    def __init__(self,corpus):\n",
    "        self.corpus = self.remove_stop_words(corpus)\n",
    "        self.tokenizer.fit_on_texts(corpus)\n",
    "        \n",
    "    def remove_stop_words(self,data):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        for i in range(len(data)):\n",
    "            sent_lst = list(data[i].split(\" \"))\n",
    "            sent_lst = filter(lambda x: x not in stop_words, sent_lst)\n",
    "            data[i] = ' '.join([str(wrd) for wrd in sent_lst])\n",
    "        return data\n",
    "            \n",
    "    def tokenize(self,data):\n",
    "        data = self.remove_stop_words(data)\n",
    "        seq = self.tokenizer.texts_to_sequences(data)\n",
    "        if(self.max_sequence_len == None):self.max_sequence_len = max([len(x) for x in seq])        \n",
    "        seq_padded = pad_sequences(seq,maxlen=self.max_sequence_len)\n",
    "        return(seq_padded)\n",
    "    \n",
    "    def get_word_index(self):\n",
    "        return(self.tokenizer.word_index)\n",
    "    \n",
    "    def get_seq_length(self):\n",
    "        return self.max_sequence_len\n",
    "    \n",
    "    def get_vocab_length(self):\n",
    "        return len(self.tokenizer.word_index) + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "pre_pro_X = PreProcess(data['description'])\n",
    "X = np.array(pre_pro_X.tokenize(data['description']))\n",
    "\n",
    "max_seq_length = pre_pro_X.get_seq_length()\n",
    "vocab_length = pre_pro_X.get_vocab_length()\n",
    "print(max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_pro_y = PreProcess(data['words'])\n",
    "y = np.array(pre_pro_y.tokenize(data['words']))\n",
    "\n",
    "total_target_words = pre_pro_y.get_vocab_length()\n",
    "ys = tf.keras.utils.to_categorical(y, num_classes=total_target_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_length, 100, input_length=36))\n",
    "\n",
    "model.add(Bidirectional(LSTM(256)))\n",
    "\n",
    "model.add(Dense(total_target_words, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"ADAM\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "290/672 [===========>..................] - ETA: 1:17 - loss: 10.3472 - accuracy: 2.1552e-04"
     ]
    }
   ],
   "source": [
    "earlystop = EarlyStopping(monitor='accuracy', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "history = model.fit(X, ys, epochs=5, verbose=1,callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history,'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_desc_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted = np.argmax(model.predict(test_desc_encoded), axis=-1)\n",
    "# wrd = 'e x _ m p _ a _'\n",
    "# test_description = \"something to be imitated\"\n",
    "\n",
    "# test_desc_encoded = pre_pro_X.tokenize([test_description])\n",
    "# preds = list(model.predict(test_desc_encoded).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predict:\n",
    "    \n",
    "    final_preds = {\"Masked\":[],\"preds\":[]}\n",
    "    \n",
    "    def __init__(self,target_words,model,pre_pro_X,pre_pro_y):\n",
    "        self.target_words =  target_words\n",
    "        self.model        =  model\n",
    "        self.pre_pro_X    =  pre_pro_X\n",
    "        self.pre_pro_Y    =  pre_pro_Y\n",
    "        \n",
    "    def get_words_set(pred_word,dashed,wrd_len):\n",
    "        words_set = filter(lambda word : nltk.edit_distance(word, pred_word) ==  dashed and wrd_len ==  len(word),\n",
    "                          self.target_words)\n",
    "        return words_set\n",
    "        \n",
    "    def predict_word(wrd,preds):\n",
    "        wrd_lst = wrd.split(\" \")\n",
    "        wrd_conv = ''.join(x for x in filter(lambda x: x!='_', wrd_lst))\n",
    "        \n",
    "        words_set = self.get_words_set(wrd_conv,wrd_lst.count('_'),len(wrd_lst))\n",
    "        \n",
    "        target_wrds_set = list(self.pre_pro_y.tokenizer.word_index.keys())\n",
    "        target_wrds_inxs, selective_preds = [],[]\n",
    "\n",
    "        for word in words_set:\n",
    "            try:target_wrds_inxs.append(word)\n",
    "            except:pass    \n",
    "\n",
    "        for inx in target_wrds_inxs:\n",
    "            try:selective_preds.append(preds[target_wrds_set.index(inx)])\n",
    "            except:pass\n",
    "\n",
    "        max_prob = max(selective_preds)\n",
    "        return(target_wrds_set[preds.index(max_prob)])\n",
    "                \n",
    "    \n",
    "    def predict_words(words_dataframe):\n",
    "        \n",
    "        for index,row in words_dataframe:\n",
    "            self.final_preds[\"Masked\"].append(row[\"Masked\"])\n",
    "            \n",
    "            test_desc_encoded = self.pre_pro_X.tokenize([row['Meaning']])\n",
    "            preds = list(self.model.predict(test_desc_encoded).reshape(-1))\n",
    "            \n",
    "            self.final_preds[\"preds\"].append(self.predict_word(row[\"Masked\"]))\n",
    "                   \n",
    "\n",
    "                \n",
    "predict = Predict(target_words, model, pre_pro_X, pre_pro_y) \n",
    "print(predict.predict_words(pd.DataFrame({\"Masked\":['e x _ m p _ a _'],\"Meaning\":[\"something to be imitated\"]})))\n",
    "                \n",
    "# def get_words_set(pred_word,dashed,wrd_len):\n",
    "#     words_set  = []\n",
    "#     for word in target_words:\n",
    "#         if(nltk.edit_distance(word, pred_word)== dashed and wrd_len ==  len(word)):\n",
    "#             words_set.append(word)\n",
    "#     return(words_set)\n",
    "\n",
    "# def predict_words(wrd, preds):\n",
    "#     wrd_lst = wrd.split(' ')\n",
    "#     wrd_len = len(wrd_lst)\n",
    "#     wrd_lst= filter(lambda x: x!='_',wrd_lst)\n",
    "#     wrd_cnv = ''.join(x for x in wrd_lst)\n",
    "    \n",
    "    \n",
    "# #     for wrd in wrd_lst:\n",
    "# #         if wrd!='_':wrd_conv = wrd_conv+wrd\n",
    "        \n",
    "#     words_set = get_words_set(wrd_conv,wrd_lst.count('_'),wrd_len)\n",
    "#     target_wrds_set = list(pre_pro_y.tokenizer.word_index.keys())\n",
    "#     target_wrds_inxs, selective_preds = [],[]\n",
    "    \n",
    "#     for word in words_set:\n",
    "#         try:target_wrds_inxs.append(word)\n",
    "#         except:pass    \n",
    "        \n",
    "#     for inx in target_wrds_inxs:\n",
    "#         try:selective_preds.append(preds[target_wrds_set.index(inx)])\n",
    "#         except:pass\n",
    "\n",
    "#     max_prob = max(selective_preds)\n",
    "#     return(target_wrds_set[preds.index(max_prob)])\n",
    "   \n",
    "#print(predict_words(wrd,preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrd = 'g _ m'\n",
    "test_description = \"a herd of whales\"\n",
    "\n",
    "test_desc_encoded = pre_pro_X.tokenize([test_description])\n",
    "preds = list(model.predict(test_desc_encoded).reshape(-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
